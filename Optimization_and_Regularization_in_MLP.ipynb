{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaAPpHKmGs7U"
      },
      "source": [
        "# **Install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRHtZDleQhzW"
      },
      "outputs": [],
      "source": [
        "!pip install progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjpDcUeAHlnr"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iniYmPWSGu18"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import logsumexp\n",
        "import matplotlib.pyplot as plt\n",
        "from progress.bar import IncrementalBar\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnBBhGYBHVY_"
      },
      "source": [
        "# **Activation Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9issr7lHPMRZ"
      },
      "outputs": [],
      "source": [
        "# derivative function, gets function output as input and computer derivative\n",
        "class Linear:\n",
        "    def __init__(self):\n",
        "        self.__name__ = 'linear'\n",
        "\n",
        "    def __call__(self, Z):\n",
        "        return Z\n",
        "\n",
        "    def derivative(self, Z, *args, **kwargs):\n",
        "        return 1\n",
        "\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.__name__ = 'relu'\n",
        "\n",
        "    def __call__(self, Z):\n",
        "       \n",
        "        return Z * (Z > 0)\n",
        "\n",
        "    def derivative(self, Z, *args, **kwargs):\n",
        "        return (Z > 0) * 1\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.__name__ = 'sigmoid'\n",
        "\n",
        "    def __call__(self, Z):\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "    def derivative(self, Z, *args, **kwargs):\n",
        "        return Z * (1 - Z)\n",
        "\n",
        "\n",
        "\n",
        "# stable version of softmax:\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.__name__ = 'softmax'\n",
        "\n",
        "    def __call__(self, Z):\n",
        "        e = -np.max(Z, axis=1, keepdims=True)\n",
        "        exp = np.exp(Z + e)\n",
        "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n",
        "    def derivative(self, Z, *args, **kwargs):\n",
        "        raise Exception('cannot get derivative')\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "    def __init__(self):\n",
        "        self.__name__ = 'tanh'\n",
        "\n",
        "    def __call__(self, Z):\n",
        "        return np.tanh(Z)\n",
        "\n",
        "    def derivative(self, Z, *args, **kwargs):\n",
        "        return 1 - (np.tanh(Z) ** 2)\n",
        "\n",
        "\n",
        "sigmoid = Sigmoid\n",
        "softmax = Softmax\n",
        "relu = Relu\n",
        "linear = Linear\n",
        "tanh = Tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4IOd9sgPbQD"
      },
      "outputs": [],
      "source": [
        "class Static_split:\n",
        "    def __init__(self, test_split=0.2):\n",
        "        self.test_split = test_split\n",
        "\n",
        "    def set_data(self, X, y, shuffle=True):\n",
        "        if shuffle == False:\n",
        "            p = np.arange(X.shape[0])\n",
        "        else:\n",
        "            p = np.random.permutation(X.shape[0])\n",
        "\n",
        "        split = int(X.shape[0] * self.test_split)\n",
        "        self.X_test = X[p[:split]]\n",
        "        self.y_test = y[p[:split]]\n",
        "        self.X_train = X[p[split:]]\n",
        "        self.y_train = y[p[split:]]\n",
        "\n",
        "    def get_test_data(self):\n",
        "        return self.X_test, self.y_test\n",
        "\n",
        "    def get_train_data(self):\n",
        "        return self.X_train, self.y_train\n",
        "\n",
        "\n",
        "class kfold_split:\n",
        "    def __init__(self, k=5):\n",
        "        self.k = k\n",
        "\n",
        "    def set_data(self, X, y, shuffle=True):\n",
        "        if shuffle == False:\n",
        "            p = np.arange(X.shape[0])\n",
        "        else:\n",
        "            p = np.random.permutation(X.shape[0])\n",
        "\n",
        "        self.split = np.random.randint(0, self.k, size=X.shape[0])\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.__next__()\n",
        "\n",
        "    def __next__(self):\n",
        "        for i in range(self.k):\n",
        "            yield self.X[self.split != i], self.y[self.split != i], self.X[self.split == i], self.y[self.split == i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zymjNV5IY40"
      },
      "source": [
        "# **Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WctzL2vRPhCK"
      },
      "outputs": [],
      "source": [
        "class Dense:\n",
        "    def __init__(self, unit, activation=linear, input_dim=0, trainable=True):\n",
        "        self.unit = unit\n",
        "        self.__name__ = 'Dense'\n",
        "        self.name = \"dense\"\n",
        "        self.trainable = trainable\n",
        "        if type(activation) == str:\n",
        "            if activation.lower() == 'linear':\n",
        "                activation = Linear()\n",
        "            elif activation.lower() == 'sigmoid':\n",
        "                activation = Sigmoid()\n",
        "            elif activation.lower() == 'relu':\n",
        "                activation = Relu()\n",
        "            elif activation.lower() == 'softmax':\n",
        "                activation = Softmax()\n",
        "            elif activation.lower() == 'tanh':\n",
        "                activation = Tanh()\n",
        "            else:\n",
        "                raise Exception(\"Activation function \" + activation + \" not found!\")\n",
        "\n",
        "        self.activation = activation\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "    def set_name(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def compile(self, input_dim=-1):\n",
        "        if input_dim == -1:\n",
        "            assert self.input_dim != 0\n",
        "        else:\n",
        "            self.input_dim = input_dim\n",
        "\n",
        "        self.W = (np.random.random(size=(self.input_dim + 1, self.unit))) - 0.5\n",
        "        self.W = self.W / (self.W.std() * (input_dim + self.unit) ** 0.5)\n",
        "\n",
        "    def __call__(self, X, activation=True):\n",
        "        if activation:\n",
        "            return self.activation(np.dot(np.hstack((X, np.ones((X.shape[0], 1)))), self.W))\n",
        "        else:\n",
        "            return np.dot(np.hstack((X, np.ones((X.shape[0], 1)))), self.W)\n",
        "\n",
        "    def set_trainable(self, trainable):\n",
        "        self.trainable = trainable\n",
        "\n",
        "    def update(self, dw):\n",
        "        if self.trainable:\n",
        "            self.W = self.W + dw\n",
        "\n",
        "    def reset_weights(self):\n",
        "        self.W = (np.random.random(size=self.W.shape)) - 0.5\n",
        "        self.W = self.W / (self.W.std() * (self.W.shape[0] + self.unit) ** 0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTeszbx6IlkO"
      },
      "source": [
        "# **Loss Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii7ZWNMDPlzZ"
      },
      "outputs": [],
      "source": [
        "class Cross_entropy:\n",
        "    def __init__(self, last_layer_activation):\n",
        "        if last_layer_activation == 'sigmoid':\n",
        "            self.get_loss = self.call_sigmoid\n",
        "            self.derivative = self.derivative_sigmoid\n",
        "        elif last_layer_activation == 'softmax':\n",
        "            self.get_loss = self.call_softmax\n",
        "            self.derivative = self.derivative_softmax\n",
        "\n",
        "    def call_sigmoid(self, z, y):\n",
        "        return np.mean(z * (1 - y) + np.logaddexp(0, -z))\n",
        "\n",
        "    def derivative_sigmoid(self, h, y):\n",
        "        return h - y\n",
        "\n",
        "    def call_softmax(self, z, y):\n",
        "        return -np.mean(np.sum(y * (z - logsumexp(z, axis=1, keepdims=True)), axis=1))\n",
        "\n",
        "    def derivative_softmax(self, h, y):\n",
        "        return h - y\n",
        "\n",
        "    def get_loss(self, h, y):\n",
        "        return -np.mean(np.sum(y * np.log(h) + (1 - y) * np.log(1 - h), axis=1))\n",
        "\n",
        "    def derivative(self, h, y):\n",
        "        return (h - y) / (h * (1 - h))\n",
        "\n",
        "    def __call__(self, h, y):\n",
        "        self.get_loss(h, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkmjzPSoIy7m"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtDDbkwQPuEH"
      },
      "outputs": [],
      "source": [
        "class Sequential:\n",
        "    def __init__(self):\n",
        "        self.remove_last_layer_activation = False\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.optimizer = None\n",
        "        self.regularization = None\n",
        "        self.layer_counter = 1\n",
        "\n",
        "    def add(self, layer):\n",
        "        layer.set_name('dense_' + str(self.layer_counter))\n",
        "        self.layer_counter += 1\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def initialize_layers(self):\n",
        "        input_dim = -1\n",
        "        for layer in self.layers:\n",
        "            layer.compile(input_dim)\n",
        "            input_dim = layer.unit\n",
        "\n",
        "    def compile(self, optimizer='SGD', loss='cross_entropy', regularization=None):\n",
        "        if loss.lower() == 'cross_entropy':\n",
        "            self.loss = Cross_entropy(self.layers[-1].activation.__name__)\n",
        "            if self.layers[-1].activation.__name__ == \"sigmoid\" or self.layers[-1].activation.__name__ == \"softmax\":\n",
        "                self.remove_last_layer_activation = True\n",
        "\n",
        "        if type(optimizer) == str:\n",
        "            if optimizer.lower() == 'sgd':\n",
        "                optimizer = SGD()\n",
        "            elif optimizer.lower() == 'momentum_sgd':\n",
        "                optimizer = Momentum_SGD()\n",
        "            elif optimizer.lower() == 'adagrad':\n",
        "                optimizer = Adagrad()\n",
        "            elif optimizer.lower() == 'rmsprop':\n",
        "                optimizer = RMSprop()\n",
        "            elif optimizer.lower() == 'adam':\n",
        "                optimizer = Adam()\n",
        "            else:\n",
        "                raise Exception(\"Optimizer \" + optimizer + \" not found!\")\n",
        "\n",
        "        if regularization is not None and type(regularization) == str:\n",
        "            if regularization.lower() == 'l1':\n",
        "                regularization = L1()\n",
        "            elif regularization.lower() == 'l2':\n",
        "                regularization = L2()\n",
        "\n",
        "        self.regularization = regularization\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "\n",
        "        self.initialize_layers()\n",
        "\n",
        "    def fit(self, X, y, batch_size=32, epoch=1, test_data=None):\n",
        "        assert self.loss is not None\n",
        "\n",
        "        history = {'acc': np.zeros(epoch), 'loss': np.zeros(epoch)}\n",
        "        if test_data is not None:\n",
        "            history['val_acc'] = np.zeros(epoch)\n",
        "            history['val_loss'] = np.zeros(epoch)\n",
        "\n",
        "        for epch in range(epoch):\n",
        "            print('Epoch', epch + 1)\n",
        "            sum_loss = 0\n",
        "            sum_acc = 0\n",
        "            bt_conter = 0\n",
        "\n",
        "            required_batchs = int(X.shape[0] / batch_size + 0.99999999)\n",
        "            bar = IncrementalBar(max=required_batchs,\n",
        "                                 suffix='%(index)d/%(max)d - %(eta)ds')\n",
        "\n",
        "            epoch_shufle = np.random.permutation(X.shape[0])\n",
        "\n",
        "            for bt in range(0, X.shape[0], batch_size):\n",
        "                bt_conter += 1\n",
        "                # single batch:\n",
        "                X_batch = X[epoch_shufle[bt:min(bt + batch_size, X.shape[0])]]\n",
        "                y_batch = y[epoch_shufle[bt:min(bt + batch_size, X.shape[0])]]\n",
        "\n",
        "                layer_output = [X_batch]\n",
        "                for j, layer in enumerate(self.layers):\n",
        "                    layer_output.append(layer(layer_output[-1]))\n",
        "\n",
        "                delta_next = self.loss.derivative(layer_output[-1], y_batch)\n",
        "                if not self.remove_last_layer_activation:\n",
        "                    delta_next *= self.layers[-1].activation.derivative(layer_output[-1])\n",
        "\n",
        "                grad = [0 for i in range(len(self.layers))]\n",
        "                regularization_grad = [0 for i in range(len(self.layers))]\n",
        "                if self.regularization is not None:\n",
        "                    regularization_grad = self.regularization.derivative(self.layers)\n",
        "\n",
        "                for i in range(len(self.layers) - 1, 0, -1):\n",
        "                    delta_prev = np.dot(delta_next, self.layers[i].W.T[:, :-1]) * \\\n",
        "                                 self.layers[i - 1].activation.derivative(layer_output[i])\n",
        "\n",
        "                    delta_wi = np.dot(np.hstack((layer_output[i], np.ones((layer_output[i].shape[0], 1)))).T,\n",
        "                                      delta_next)\n",
        "                    grad[i] = delta_wi / X_batch.shape[0] + regularization_grad[i]\n",
        "\n",
        "                    delta_next = delta_prev\n",
        "\n",
        "                delta_wi = np.dot(np.hstack((layer_output[0], np.ones((layer_output[0].shape[0], 1)))).T, delta_next)\n",
        "                grad[0] = delta_wi / X_batch.shape[0] + regularization_grad[0]\n",
        "\n",
        "                self.optimizer.update(grad, self.layers)\n",
        "\n",
        "                d = self.evaluate(X_batch, y_batch)\n",
        "                loss, acc = d['loss'], d['acc']\n",
        "                sum_acc += acc\n",
        "                sum_loss += loss\n",
        "                bar.next()\n",
        "                sys.stdout.write(\" loss: %f acc: %f\" % (sum_loss / (bt_conter + 1), sum_acc / (bt_conter + 1)))\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            bar.finish()\n",
        "            d = self.evaluate(X, y)\n",
        "            loss, acc = d['loss'], d['acc']\n",
        "            if test_data is not None:\n",
        "                d = self.evaluate(test_data[0], test_data[1])\n",
        "                val_loss, val_acc = d['loss'], d['acc']\n",
        "                history['val_loss'][epch] = val_loss\n",
        "                history['val_acc'][epch] = val_acc\n",
        "                history['loss'][epch] = loss\n",
        "                history['acc'][epch] = acc\n",
        "                print('loss: {} acc: {} val_loss: {} val_acc: {}'.format(loss, acc, val_loss, val_acc))\n",
        "            else:\n",
        "                history['loss'][epch] = loss\n",
        "                history['acc'][epch] = acc\n",
        "                print('loss: {} acc: {}'.format(loss, acc))\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X, batch_size=32, last_activation=True):\n",
        "        return np.argmax(self.predict_proba(X, batch_size=batch_size, last_activation=last_activation), axis=1)\n",
        "\n",
        "    def predict_proba(self, X, batch_size=32, last_activation=True):\n",
        "        output = np.zeros((X.shape[0], self.layers[-1].unit))\n",
        "\n",
        "        for i in range(0, X.shape[0], batch_size):\n",
        "            t = X[i:min(i + batch_size, X.shape[0])].copy()\n",
        "            for j, layer in enumerate(self.layers):\n",
        "                t = layer(t, activation=(j != len(self.layers) - 1 or last_activation))\n",
        "\n",
        "            output[i:min(i + batch_size, X.shape[0])] = t\n",
        "\n",
        "        return output\n",
        "\n",
        "    def evaluate(self, X, y, batch_size=32):\n",
        "        if self.loss is not None:\n",
        "            p = self.predict_proba(X, batch_size=batch_size, last_activation=not self.remove_last_layer_activation)\n",
        "            loss = self.loss.get_loss(p, y)\n",
        "            if self.regularization is not None:\n",
        "                loss += self.regularization.cost(self.layers)\n",
        "\n",
        "            if self.remove_last_layer_activation:\n",
        "                p = self.layers[-1].activation(p)\n",
        "\n",
        "            pred = np.argmax(p, axis=1)\n",
        "            y_class = np.argmax(y, axis=1)\n",
        "\n",
        "            acc = np.sum(pred == y_class) / p.shape[0]\n",
        "            return {'loss': loss, 'acc': acc}\n",
        "        else:\n",
        "            p = self.predict_proba(X, batch_size=batch_size)\n",
        "\n",
        "            pred = np.argmax(p, axis=1)\n",
        "            y_class = np.argmax(y, axis=1)\n",
        "\n",
        "            acc = np.sum(pred == y_class) / p.shape[0]\n",
        "            return {'acc': acc}\n",
        "\n",
        "    def summary(self):\n",
        "        print(\"_\" * 65)\n",
        "        print('Layer (type)                 Output Shape              Param #')\n",
        "        print(\"=\" * 65)\n",
        "\n",
        "        layer_type = 'Input'\n",
        "        output_shape = '(None, {})'.format(str(self.layers[0].input_dim))\n",
        "        params = '0'\n",
        "        print('{:<29}'.format(layer_type) +\n",
        "              '{:<26}'.format(output_shape) +\n",
        "              '{}'.format(params))\n",
        "        print(\"-\" * 65)\n",
        "\n",
        "        trainable_params = 0\n",
        "        non_trainable_params = 0\n",
        "\n",
        "        for l in self.layers:\n",
        "            layer_type = l.name + ' ({})'.format(l.__name__)\n",
        "            output_shape = '(None, {})'.format(str(l.unit))\n",
        "            params = '{}'.format(l.W.shape[0] * l.W.shape[1])\n",
        "\n",
        "            if l.trainable:\n",
        "                trainable_params += l.W.shape[0] * l.W.shape[1]\n",
        "            else:\n",
        "                non_trainable_params += l.W.shape[0] * l.W.shape[1]\n",
        "\n",
        "            print('{:<29}'.format(layer_type) +\n",
        "                  '{:<26}'.format(output_shape) +\n",
        "                  '{}'.format(params))\n",
        "            print(\"-\" * 65)\n",
        "\n",
        "        print('Total params:', trainable_params + non_trainable_params)\n",
        "        print('Trainable params:', trainable_params)\n",
        "        print('Non-trainable params:', non_trainable_params)\n",
        "        print(\"_\" * 65)\n",
        "        print('Optimizer : {}'.format(self.optimizer.__name__))\n",
        "        if self.regularization is not None:\n",
        "            print('Regularization : {}'.format(self.regularization.__name__))\n",
        "        print(\"-\" * 65)\n",
        "        print()\n",
        "        print()\n",
        "\n",
        "    def save_weights(self, path):\n",
        "        import os\n",
        "        try:\n",
        "            os.rmdir(path)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        os.mkdir(path)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            np.save(os.path.join(path, str(i) + '.npy'), layer.W)\n",
        "\n",
        "    def load_weights(self, path):\n",
        "        import os\n",
        "        assert len(os.listdir(path)) == len(self.layers)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            self.layers[i].W = np.load(os.path.join(path, str(i) + '.npy'))\n",
        "\n",
        "    def reset_weights(self):\n",
        "        for layer in self.layers:\n",
        "            layer.reset_weights()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJOfho0JMIsC"
      },
      "source": [
        "# === Regularization ==="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gW7zNNVMIsE"
      },
      "source": [
        "## **L1 Regularization**\n",
        "complete this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFhnsblvQLXJ"
      },
      "outputs": [],
      "source": [
        "class L1:\n",
        "    def __init__(self, lam=0.001):\n",
        "        self.lam = lam\n",
        "        self.__name__ = 'L1 Regularization'\n",
        "\n",
        "    def cost(self, layers):\n",
        "        # define L1 Regularization Cost function here\n",
        "        # input: layers -> list of layers. you can access the weight matrix of i'th layer\n",
        "        #                  using \"layers[i].W\".\n",
        "        #                  layers[i].W[-1, :] is the bias term and layers[i].W[:-1, :]\n",
        "        #                  will give you all weights\n",
        "        #\n",
        "        # output: one float number. this number is the total cost of the Regularization term.\n",
        "        #         don't forget to effect \"self.lam\" in the return value. lam is the\n",
        "        #         Regularization Coefficient.\n",
        "\n",
        "        cost = 0\n",
        "        return cost * self.lam\n",
        "\n",
        "        # define L1 Regularization Cost function here\n",
        "\n",
        "    def derivative(self, layers):\n",
        "        # define L1 Regularization Cost function Derivatives here\n",
        "        # input: layers -> list of layers. you can access weight matrix of i'th layer\n",
        "        #                  using \"layers[i].W\".\n",
        "        #                  layers[i].W[-1, :] is the bias term and layers[i].W[:-1, :]\n",
        "        #                  will give you all weights\n",
        "        #\n",
        "        # output: a list of matrices. output[i] is a matrix with same shapes of layers[i].W\n",
        "        #         matrix and indicates derivative of each weight respect to the Regularization term\n",
        "        #\n",
        "\n",
        "        return [np.zeros(l.W.shape) * self.lam for l in layers]\n",
        "\n",
        "        # define L1 Regularization Cost function Derivatives here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkWlIoqpMsKc"
      },
      "source": [
        "## **L2 Regularization**\n",
        "complete this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtBwv0jUMkS6"
      },
      "outputs": [],
      "source": [
        "class L2:\n",
        "    def __init__(self, lam=0.001):\n",
        "        self.lam = lam\n",
        "        self.__name__ = 'L2 Regularization'\n",
        "\n",
        "    def cost(self, layers):\n",
        "        # define L2 Regularization Cost function here\n",
        "        # input: layers -> list of layers. you can access the weight matrix of i'th layer\n",
        "        #                  using \"layers[i].W\".\n",
        "        #                  layers[i].W[-1, :] is the bias term and layers[i].W[:-1, :]\n",
        "        #                  will give you all weights\n",
        "        #\n",
        "        # output: one float number. this number is the total cost of the Regularization term.\n",
        "        #         don't forget to effect \"self.lam\" in the return value. lam is the\n",
        "        #         Regularization Coefficient.\n",
        "\n",
        "        cost = 0\n",
        "        return cost * self.lam\n",
        "\n",
        "        # define L2 Regularization Cost function here\n",
        "\n",
        "    def derivative(self, layers):\n",
        "        # define L2 Regularization Cost function Derivatives here\n",
        "        # input: layers -> list of layers. you can access weight matrix of i'th layer\n",
        "        #                  using \"layers[i].W\".\n",
        "        #                  layers[i].W[-1, :] is the bias term and layers[i].W[:-1, :]\n",
        "        #                  will give you all weights\n",
        "        #\n",
        "        # output: a list of matrices. output[i] is a matrix with same shapes of layers[i].W\n",
        "        #         matrix and indicates derivative of each weight respect to the Regularization term\n",
        "        #\n",
        "\n",
        "        return [np.zeros(l.W.shape) * self.lam for l in layers]\n",
        "\n",
        "        # define L2 Regularization Cost function Derivatives here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7E1MJGxJIoH"
      },
      "source": [
        "# === Optimizers ==="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROwPUhXMJSe7"
      },
      "source": [
        "## **SGD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xKYaChnQFrS"
      },
      "outputs": [],
      "source": [
        "# SGD Optimizer as a template to see how you can write Optimizers.\n",
        "# Calling layer.update(delta) method, will simply update weight matrix\n",
        "# by formula:\n",
        "# w_new = w_old + delta\n",
        "# All classes have a dummy output to prevent errors. the output is just SGD update rule\n",
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.__name__ = 'SGD'\n",
        "\n",
        "    def update(self, grads, layers):\n",
        "        for layer, delta in zip(layers, grads):\n",
        "            layer.update(-delta * self.lr)\n",
        "\n",
        "sgd = SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp07Fu2UJwfq"
      },
      "source": [
        "### **Test SGD**\n",
        "complete load_dataset() function and run this block to test SGD optimizer.\n",
        "\n",
        "You have to test 3 different models with different number of neurons in hidden layer,  you can test this part here or later in next blocks.\n",
        "\n",
        "Donot forget to save these 3 tests in the notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dALI0VPMOFLO"
      },
      "outputs": [],
      "source": [
        "#you should load data, normalize and split into train, validation and test sets.\n",
        "def load_dataset():\n",
        "    pass\n",
        "\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_dataset()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(75, 'relu', input_dim=X_train.shape[1])) # hidden layer, 75 neurons with relu activation function\n",
        "model.add(Dense(10, 'softmax')) # output layer, 10 neurons with softmax activation function\n",
        "\n",
        "\n",
        "model.compile(loss='cross_entropy', optimizer='sgd', regularization='l2')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "split_data = Static_split(test_split=0.2)\n",
        "split_data.set_data(X_train, y_train)\n",
        "(X_train_new, y_train_new) = split_data.get_train_data()\n",
        "(X_val, y_val) = split_data.get_test_data()\n",
        "\n",
        "history = model.fit(X_train_new, y_train_new, epoch=20,\n",
        "                    batch_size=64, test_data=(X_val, y_val))\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('acc')\n",
        "plt.plot(history['acc'], 'b-o', label='Train')\n",
        "plt.plot(history['val_acc'], 'r--o', label='Test')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('loss')\n",
        "plt.plot(history['loss'], 'b-o', label='Train')\n",
        "plt.plot(history['val_loss'], 'r--o', label='Test')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHWxIUnGKR1a"
      },
      "source": [
        "## **Momentum SGD**\n",
        "complete this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Hsr_ZBJ7pX"
      },
      "outputs": [],
      "source": [
        "class Momentum_SGD(SGD):\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        super().__init__(lr)\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Helping attribute\n",
        "        self.velocity = None\n",
        "\n",
        "        self.__name__ = 'Momentum_SGD'\n",
        "\n",
        "    def update(self, grads, layers):\n",
        "        # define Momentum_SGD update rule here.\n",
        "        # inputs are grads and layers.\n",
        "        #        grads is a list of gradient matrices.\n",
        "        #        layers is a list of layers.\n",
        "        #        you may need to define new methods(functions) or new attributes(variables)\n",
        "\n",
        "        for layer, delta in zip(layers, grads):\n",
        "            layer.update(-delta * self.lr)\n",
        "\n",
        "        # define Momentum_SGD update rule here.\n",
        "\n",
        "momentum_sgd = Momentum_SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IPoFVZwKR1d"
      },
      "source": [
        "**### Test Momentum SGD**\n",
        "\n",
        "complete and run this block to test optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJvY4PEFKcnO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ebuc38CKqFj"
      },
      "source": [
        "## **Adagrad**\n",
        "complete this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDcZmuS8Kn8O"
      },
      "outputs": [],
      "source": [
        "class Adagrad:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.global_lr = lr\n",
        "        self.epsilon = 1e-7\n",
        "        self.velocity = None\n",
        "        self.accumulated_gradient = None\n",
        "\n",
        "        self.__name__ = 'AdaGrad'\n",
        "\n",
        "    def update(self, grads, layers):\n",
        "        # define AdaGrad update rule here.\n",
        "        # inputs are grads and layers.\n",
        "        #        grads is a list of gradient matrices.\n",
        "        #        layers is a list of layers.\n",
        "        #        you may need to define new methods(functions) or new attributes(variables)\n",
        "\n",
        "        for layer, delta in zip(layers, grads):\n",
        "            layer.update(-delta * self.global_lr)\n",
        "\n",
        "        # define AdaGrad update rule here.\n",
        "adagrad = Adagrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14uX49bHKqFm"
      },
      "source": [
        "**### Test Adagrad**\n",
        "\n",
        "complete and run this block to test optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIkdiDUeLRT7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNihbxOALTNN"
      },
      "source": [
        "## **RMSprop**\n",
        "complete this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XWD4vPNLCU3"
      },
      "outputs": [],
      "source": [
        "class RMSprop:\n",
        "    def __init__(self, lr=0.001, rho=0.9):\n",
        "        self.global_lr = lr\n",
        "        self.rho = rho\n",
        "        self.epsilon = 1e-7\n",
        "\n",
        "        # Helping attribute\n",
        "        self.velocity = None\n",
        "        self.accumulated_gradient = None\n",
        "\n",
        "        self.__name__ = 'RMSprop'\n",
        "\n",
        "    def update(self, grads, layers):\n",
        "        # define RMSprop update rule here.\n",
        "        # inputs are grads and layers.\n",
        "        #        grads is a list of gradient matrices.\n",
        "        #        layers is a list of layers.\n",
        "        #        you may need to define new methods(functions) or new attributes(variables)\n",
        "\n",
        "        for layer, delta in zip(layers, grads):\n",
        "            layer.update(-delta * self.global_lr)\n",
        "\n",
        "        # define RMSprop update rule here.\n",
        "rmsprop = RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbo2GxduLTNO"
      },
      "source": [
        "**### Test RMSprop**\n",
        "\n",
        "complete and run this block to test optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrHNzWi8Lao_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egmYFpqgLkyG"
      },
      "source": [
        "## **Adam**\n",
        "\n",
        "complete this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky1noJYSLieC"
      },
      "outputs": [],
      "source": [
        "class Adam(Adagrad):\n",
        "    def __init__(self, lr=0.001, rho_1=0.9, rho_2=0.999):\n",
        "        self.global_lr = lr\n",
        "        self.rho_1 = rho_1\n",
        "        self.rho_2 = rho_2\n",
        "\n",
        "        # Helping attribute\n",
        "        self.first_moment_estimate = None\n",
        "        self.second_moment_estimate = None\n",
        "        self.t = 0\n",
        "        self.__name__ = 'Adam'\n",
        "\n",
        "    def update(self, grads, layers):\n",
        "        # define Adam update rule here.\n",
        "        # inputs are grads and layers.\n",
        "        #        grads is a list of gradient matrices.\n",
        "        #        layers is a list of layers.\n",
        "        #        you may need to define new methods(functions) or new attributes(variables)\n",
        "\n",
        "        for layer, delta in zip(layers, grads):\n",
        "            layer.update(-delta * self.global_lr)\n",
        "\n",
        "        # define Adam update rule here.\n",
        "\n",
        "adam = Adam        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp26Q_O8LkyI"
      },
      "source": [
        "**### Test Adam**\n",
        "\n",
        "complete and run this part to test optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzttPOehLslS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6r2aEluQrT8"
      },
      "source": [
        "\n",
        "**Test Best Optimizer with another regularization**\n",
        "\n",
        "please use a regularization which hasnot been used already!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBGGSzdSQ_Dy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLaRWe9rRAq9"
      },
      "source": [
        "# Test best model on test data\n",
        "\n",
        "use your best model and run that on test dataset, report final accuracy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
