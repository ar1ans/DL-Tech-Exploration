# Deep Learning Optimization and Regularization Techniques

This repository contains a Jupyter Notebook detailing a university homework assignment focused on implementing and comparing various optimization and regularization techniques in a Multi-Layer Perceptron (MLP) using the Fashion MNIST dataset. Created by Aryan Sabounchi, this project aims to explore the effectiveness of different methods in improving model accuracy and generalization.

## Overview

The project involves:
- Implementing optimization techniques such as SGD, Momentum, Adagrad, RMSProp, and Adam.
- Applying regularization methods like L1 and L2 regularization to prevent overfitting.
- Training an MLP with a single hidden layer on the Fashion MNIST dataset, analyzing the impact of different neuron counts in the hidden layer, and selecting the best architecture based on validation accuracy.

## Installation

1. Ensure you have Jupyter Notebook installed or use Google Colab.
2. Clone this repository to access the `.ipynb` file.
3. Install required Python packages: `pip install -r requirements.txt` (requirements file to be created based on the notebook's imports).

## Usage

Open the `Optimization_and_Regularization_in_MLP.ipynb` notebook in Jupyter or Colab and follow the instructions within to complete the exercises. Parts of the code are provided, but you'll need to implement several optimization and regularization techniques as specified.

## Contributing

Contributions to improve the code or extend the analysis are welcome. Please feel free to fork the repository and submit pull requests.

## License

This project is open-sourced under the MIT License.

## Acknowledgements

- Special thanks to the instructors and peers at the university for their guidance and support.
- Dataset: Fashion MNIST from Zalando's research department (https://github.com/zalandoresearch/fashion-mnist).
